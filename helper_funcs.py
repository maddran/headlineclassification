# -*- coding: utf-8 -*-
"""Headline_classification_BERTML_AG_News.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VXt2DfQ7zrNwK08iq_kWPyDq4yUCj-Ce
"""

import subprocess
import sys

def install_and_import(package):
    import importlib
    try:
        importlib.import_module(package)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    finally:
        globals()[package] = importlib.import_module(package)


install_and_import('Sentencepiece')
install_and_import('transformers')

   

from transformers import XLMRobertaTokenizer

import torch 
from torch.utils.data import DataLoader
from transformers import XLMRobertaForSequenceClassification, AdamW

tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')

def encode_text(text, tokenizer):
  return tokenizer(text, 
            max_length=128,
            add_special_tokens = True,
            truncation=True, 
            padding='max_length')
            
class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)
            
def accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

import time
import datetime
def format_time(elapsed):
    '''
    Takes a time in seconds and returns a string hh:mm:ss
    '''
    # Round to the nearest second.
    elapsed_rounded = int(round((elapsed)))
    
    # Format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))
    


device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

def predict(data_loader, model):
  predictions = []
  true_labels = []

  for batch in data_loader:
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)

    with torch.no_grad(): 
      outputs = outputs = model(input_ids, 
                                attention_mask=attention_mask, 
                                token_type_ids=None) 

    logits = outputs[0]                   
    logits = logits.detach().cpu().numpy()
    label_ids = labels.to('cpu').numpy()

    predictions.append(logits)
    true_labels.append(label_ids)

  return predictions, true_labels
