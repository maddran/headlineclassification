{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Headline_classification_XLMRoberta_BBC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONo5rH5VFiay8/8iSCoh+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29133b63162a4193a03e246901c5273f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_791440e30a5e443185e4cfc8ee8394a3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c55e470800904f4684ecabd7f80d4788",
              "IPY_MODEL_546ea80b4ec94601944d56f80bab5ad6"
            ]
          }
        },
        "791440e30a5e443185e4cfc8ee8394a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c55e470800904f4684ecabd7f80d4788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f8bb9e23ce5483ca177f97778ef760d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5069051,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5069051,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c320a32885e4196a0825d5b22ab3f25"
          }
        },
        "546ea80b4ec94601944d56f80bab5ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_afcf03d53cb24f018f2bd39e8a0e997d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.07M/5.07M [00:01&lt;00:00, 4.86MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fef2d63392d4a7cbb06a57d54003a7c"
          }
        },
        "9f8bb9e23ce5483ca177f97778ef760d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c320a32885e4196a0825d5b22ab3f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afcf03d53cb24f018f2bd39e8a0e997d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fef2d63392d4a7cbb06a57d54003a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d45895a599b42b08f61bfc941f5a37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7ecee594708a4e8ba60cfd16b44381e6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cbd1edf0441f451a8f8b08a745778c1b",
              "IPY_MODEL_4438200d4dce4d7e8563988deec47c99"
            ]
          }
        },
        "7ecee594708a4e8ba60cfd16b44381e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbd1edf0441f451a8f8b08a745778c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f41e16e2e0134de4b00cbd4b656d22f6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 512,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 512,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_719cb8d737444f57822d890b92a2acda"
          }
        },
        "4438200d4dce4d7e8563988deec47c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0eaf3e5dbf44acfb5e659793bf2119f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 512/512 [00:01&lt;00:00, 510B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6be46dba46ea4c6d9548186439a85dc0"
          }
        },
        "f41e16e2e0134de4b00cbd4b656d22f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "719cb8d737444f57822d890b92a2acda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0eaf3e5dbf44acfb5e659793bf2119f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6be46dba46ea4c6d9548186439a85dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e685101bd7634dfda5859ee0fab7a7ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_722e929e8408453ea000fd254a03ccc7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c3fbb8956b324375a03e1fdf2645e9c4",
              "IPY_MODEL_ba926687ca0e417d98be71f97f4684a2"
            ]
          }
        },
        "722e929e8408453ea000fd254a03ccc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3fbb8956b324375a03e1fdf2645e9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_08556bae90c44ce596f54818eccd355f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1115590446,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1115590446,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d478873a23948e7990985d8a7315f12"
          }
        },
        "ba926687ca0e417d98be71f97f4684a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59f9db8cbf424a69914aadedf1fbe9e9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.12G/1.12G [05:09&lt;00:00, 3.60MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3af0995287fc4caf98019f7ab4926b26"
          }
        },
        "08556bae90c44ce596f54818eccd355f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d478873a23948e7990985d8a7315f12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59f9db8cbf424a69914aadedf1fbe9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3af0995287fc4caf98019f7ab4926b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maddran/headlineclassification/blob/main/Headline_classification_XLMRoberta_BBC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC03-8MQcJYQ"
      },
      "source": [
        "# News Topic Classification Using Transformers\n",
        "\n",
        "This blog post (or Colab notebook) is a demontration of applying [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) to a pretrained transformer model. Here, we will start with a sequence classification model provided by [Hugging Face](https://huggingface.co/models), and fine-tune it using a dataset of [BBC News content](http://mlg.ucd.ie/datasets/bbc.html) to produce a news topic classification pipeline.\n",
        "\n",
        "**N.B.** - this demonstration assumes you have some basic background in Natural Language Processing (NLP) and Deep Learning (DL), and as such, I will not be explaining the methods used in great detail. That said, I will ember links to material that could help you along the way.\n",
        "\n",
        "**N.B.** - portions of the code that follows is heavily based of the following sources:\n",
        "\n",
        "* Hugging Face examples on [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html).\n",
        "* [This post](https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1) by Medium user Aniruddha Choudhury.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35b06jtmtl1-"
      },
      "source": [
        "## Install Modules\n",
        "\n",
        "Let's start by installing two requisite modules:\n",
        "\n",
        "\n",
        "*   **Sentencepiece** - Python implementation of [Google's unsupervised text tokenizer](https://github.com/google/sentencepiece). This is only required if the transformer model chosen uses SentencePiece tokenization.\n",
        "*   **transformers** - [Hugging Face's library](https://huggingface.co/transformers/index.html) which contains Python implementations of many of the widely adopted NLP models along with a host of useful helper functions for pre-processing, training, pipelining, and more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A41GmBnZhQck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cce96df-e640-4427-e83e-c550a5e65760"
      },
      "source": [
        "!pip install -Uq Sentencepiece\n",
        "!pip install -Uq transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2MB 7.5MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.9MB 7.7MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 37.7MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.2MB 54.6MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXfvWWvTsXCL"
      },
      "source": [
        "## Download BBC data\n",
        "\n",
        "Now, lets download and parse the [BBC Dataset](http://mlg.ucd.ie/datasets/bbc.html) into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyjHEC7rhxJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002429ff-5a3c-409f-ca42-2ae363b1a92a"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "\n",
        "# Check if file has already been downloaded\n",
        "if \"bbc-fulltext.zip\" not in glob(\"bbc*.zip\"):\n",
        "  !wget \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\"\n",
        "  !unzip -q bbc*.zip\n",
        "\n",
        "# Function to extract and decode text from each file\n",
        "def get_text(filepath, headline = False):\n",
        "  with open(filepath, 'rb') as f:\n",
        "    if headline:\n",
        "      text = f.readline()\n",
        "    else:\n",
        "      text = f.read()\n",
        "  \n",
        "  return text.decode('utf-8', 'replace')\n",
        "\n",
        "path = \"bbc\"\n",
        "# Get all subfolders\n",
        "subfolders = [f.path for f in os.scandir(path) if f.is_dir()]\n",
        "res = []\n",
        "\n",
        "# Iterate through each subfolder and get text\n",
        "for sf in subfolders:\n",
        "  # get news category from fiel path\n",
        "  category = sf.split('/')[-1]\n",
        "  glob_pattern = os.path.join(f'bbc/{category}', '*')\n",
        "  # get filepaths of all files in subfolder\n",
        "  filepaths = sorted(glob(glob_pattern), key=os.path.getctime)\n",
        "  # call get_text for each filepath\n",
        "  res.append([{\"category\":category, \"text\":get_text(fp)} \n",
        "              for fp in filepaths])\n",
        "\n",
        "# Flatten resulting list of dictionaries    \n",
        "res = [item for sublist in res for item in sublist]\n",
        "\n",
        "# Create DataFrame\n",
        "bbc_data = pd.DataFrame(res)\n",
        "# Replace linebreaks with spaces\n",
        "bbc_data['text'] = bbc_data['text'].replace(r'\\n',' ', regex=True) \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-03 20:03:32--  http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\n",
            "Resolving mlg.ucd.ie (mlg.ucd.ie)... 137.43.93.132\n",
            "Connecting to mlg.ucd.ie (mlg.ucd.ie)|137.43.93.132|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2874078 (2.7M) [application/zip]\n",
            "Saving to: ‚Äòbbc-fulltext.zip‚Äô\n",
            "\n",
            "bbc-fulltext.zip    100%[===================>]   2.74M  2.87MB/s    in 1.0s    \n",
            "\n",
            "2021-03-03 20:03:33 (2.87 MB/s) - ‚Äòbbc-fulltext.zip‚Äô saved [2874078/2874078]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4CuqjyBUXe8"
      },
      "source": [
        "Let's take a look at the dataset in a bit more detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "55JWPfx5UdoI",
        "outputId": "f6f0cbbc-41a9-457c-ddaf-354fad21f23f"
      },
      "source": [
        "bbc_data.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2225</td>\n",
              "      <td>2225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>5</td>\n",
              "      <td>2127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>sport</td>\n",
              "      <td>What high-definition will do to DVDs  First it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>511</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       category                                               text\n",
              "count      2225                                               2225\n",
              "unique        5                                               2127\n",
              "top       sport  What high-definition will do to DVDs  First it...\n",
              "freq        511                                                  2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByYycX5tUjbn"
      },
      "source": [
        "So, it's a relatively small dataset with 2225 entires and five categories, which are listed below along with their frequency counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzSvA6KTUh4m",
        "outputId": "17558e90-ffc7-43cb-9fe0-7f54a130bef2"
      },
      "source": [
        "bbc_data.category.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sport            511\n",
              "business         510\n",
              "politics         417\n",
              "tech             401\n",
              "entertainment    386\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yech1EIoP57Z"
      },
      "source": [
        "## Prepare and split data\n",
        "\n",
        "Put text data into a list and encode the category labels from strings into integers. That is to say, the classification model will output an integer corresponding to the topic class (well, not exactly, but let's say that for simplicity).\n",
        "\n",
        "We also store the label encoding for use during prediction tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uygVV-EqmlQH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Put string data into a list\n",
        "data = bbc_data['text'].tolist()\n",
        "\n",
        "# Encode label categories from strings to ints\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(bbc_data.category.tolist())\n",
        "# Save encoder classes to use at inference\n",
        "np.save('classes.npy', le.classes_)\n",
        "\n",
        "# Split data and labels in train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(data, labels, test_size=.2)\n",
        "# Further split train data into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wpuyba7UCHt"
      },
      "source": [
        "The encoded labels and their corresponding categories are as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV0U6drZT9cQ",
        "outputId": "1670bb7d-0c88-4a84-b44a-cb9e29b6cac8"
      },
      "source": [
        "dict(zip(range(len(le.classes_)),le.classes_))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'business', 1: 'entertainment', 2: 'politics', 3: 'sport', 4: 'tech'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K8GAWhiW1uQ"
      },
      "source": [
        "## Picking a pre-trained model\n",
        "\n",
        "Just as we encoded the topic labels for use in model training, we must also encode the text to be fed into the model. In essence, we want to convert a sequence of words or *tokens* into a sequence of numbers. As you can imagine, this is a less than trivial task.\n",
        "\n",
        "Thankfully, the `transformers` library has predefined tokenizers for each pre-trained model. All we have to do is download the tokenizer for our chosen model and apply the requisite pre-processing steps.\n",
        "\n",
        "So, what model should we pick? In this demonstration, I will start with the pre-trained [XLM-RoBERTa](https://huggingface.co/transformers/model_doc/xlmroberta.html) model which was developed and released by [Facebook AI](https://arxiv.org/pdf/1911.02116.pdf) in 2020. While we could go with any one of the pre-trained models provided by Hugging Face, I chose XLM-R because, as of writing this in February 2021, it is relatively new, and more crucially, it is a multilingual model trained on 100 different languages. \n",
        "\n",
        "Although the BBC dataset is monolingual (i.e. it only contains English language news), we should be able to use it to train the XLM-R model to classify news in any of the 100 languages it has been trained on ü§Ø.\n",
        "\n",
        "However, the downside of using such a large model is that it is, well ... large. So unless you have a multilingual dataset to classify, I strongly urge you to look to the more pared down monolingual models.\n",
        "\n",
        "## Tokenizing and encoding text\n",
        "\n",
        "Ok, so we picked the XLM-R model, let's get to it! We start by downloading the XLM-R tokenizer from Hugging Face. We tell the tokenizer to load from a pre-trained XLM-R model `xlm-roberta-base`. The function `encode_text` calls the tokenizer on the text it is fed using the following parameters:\n",
        "\n",
        "* `max_length` - the maximum length (in tokens) of each input sequence.\n",
        "* `add_special_tokens` - adds beginning and end of sequence tokens as required by the model.\n",
        "* `truncation` - truncates each sequence to `max_length`\n",
        "* `padding` - pads each sequence to `max_length`\n",
        "\n",
        "The result is a dataset of encoded sequences of uniform length, ready to be used in model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI68czU4hz16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "29133b63162a4193a03e246901c5273f",
            "791440e30a5e443185e4cfc8ee8394a3",
            "c55e470800904f4684ecabd7f80d4788",
            "546ea80b4ec94601944d56f80bab5ad6",
            "9f8bb9e23ce5483ca177f97778ef760d",
            "6c320a32885e4196a0825d5b22ab3f25",
            "afcf03d53cb24f018f2bd39e8a0e997d",
            "7fef2d63392d4a7cbb06a57d54003a7c"
          ]
        },
        "outputId": "c0d2d35a-1b45-498b-9730-a430d8381f0f"
      },
      "source": [
        "# Download the tokenizer for the model being used\n",
        "from transformers import XLMRobertaTokenizer\n",
        "print(\"Downloading Tokenizer...\")\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "# Tokenizes and encodes the text using \n",
        "# the tokenizer loaded above\n",
        "def encode_text(text, tokenizer):\n",
        "  return tokenizer(text, \n",
        "            max_length=128, # Could use longer or shorter max lenghts\n",
        "            add_special_tokens = True, # Adds end of sequence and start of sequence tokens\n",
        "            truncation=True, # Truncates sequences to max_length\n",
        "            padding='max_length')  # Pads sequences to max_length\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29133b63162a4193a03e246901c5273f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript‚Ä¶"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpllvRelkIQQ"
      },
      "source": [
        "Let's get a better handle on what an encoded sentence looks like by running some sample sentences in different languages through `encode_text`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bq4lRpbkQl2",
        "outputId": "0c8e2068-5b85-4fb3-c17a-c8e3d54736de"
      },
      "source": [
        "# sample sequences in English, Arabic, and French\n",
        "# All credit to Will Smith, DJ Jazzy Jeff and Google Translate\n",
        "sample_text = ['Here it is the groove slightly transformed', \n",
        "        'ŸÅŸÇÿ∑ ŸÇŸÑŸäŸÑÿß ŸÖŸÜ ŸÉÿ≥ÿ± ŸÖŸÜ ÿßŸÑŸÇÿßÿπÿØÿ©',\n",
        "        'Juste un petit quelque chose pour briser la monotonie']\n",
        "\n",
        "sample_encoded = [encode_text(text, tokenizer) for text in sample_text]\n",
        "\n",
        "for i, encoded in enumerate(sample_encoded):\n",
        "  print(\"Input :\",sample_text[i])\n",
        "  print(\"Output :\",encoded['input_ids'][0:25],\"...\")\n",
        "  print(f\"Length of output : {len(encoded['input_ids'])}\")\n",
        "  print(f\"Length of output w/o padding : {len([e for e in encoded['input_ids'] if e != 1])}\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : Here it is the groove slightly transformed\n",
            "Output : [0, 11853, 442, 83, 70, 11969, 8206, 161549, 27198, 297, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ...\n",
            "Length of output : 128\n",
            "Length of output w/o padding : 11\n",
            "\n",
            "Input : ŸÅŸÇÿ∑ ŸÇŸÑŸäŸÑÿß ŸÖŸÜ ŸÉÿ≥ÿ± ŸÖŸÜ ÿßŸÑŸÇÿßÿπÿØÿ©\n",
            "Output : [0, 7012, 185997, 230, 6, 135154, 230, 207934, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ...\n",
            "Length of output : 128\n",
            "Length of output w/o padding : 9\n",
            "\n",
            "Input : Juste un petit quelque chose pour briser la monotonie\n",
            "Output : [0, 9563, 13, 51, 10174, 38944, 19667, 578, 14799, 2189, 21, 182902, 478, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ...\n",
            "Length of output : 128\n",
            "Length of output w/o padding : 14\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkDtAmLdmfvb"
      },
      "source": [
        "So, all of the input text sequences have been converted to integer sequences. A few things to note:\n",
        "\n",
        "* Each output sequence starts with a `0` (which indicates the beginning),\n",
        "* Followed a several integers numbers then a `2` (which indicates the end),\n",
        "* Finally followed by a series of `1`s to pad the sequence to the defined length of 128 characters.\n",
        "* The length of the output sequence excluding padding is not necessarily equal to the  number of words in the input sequence + 2 (for the begin and end tokens). This is due to the way SentencePiece uses subword splitting to more efficiently represent a large vocabulary of words ([see here](https://www.aclweb.org/anthology/D18-2012/) for details).\n",
        "\n",
        "That seems to work! Let's apply it to the split BBC dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGGqjDAPkQTB"
      },
      "source": [
        "texts = dict(train=train_texts, val=val_texts, test=test_texts)\n",
        "encoded_text = {key : encode_text(value, tokenizer) for key, value in texts.items()}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68kwxDvmwsI2"
      },
      "source": [
        "Before we continue, please note that much of the remaining code is heavily based of the following sources:\n",
        "\n",
        "* Hugging Face examples on [Fine-tuning with custom datasets](https://huggingface.co/transformers/custom_datasets.html).\n",
        "* [This post](https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1) by Medium user Aniruddha Choudhury.\n",
        "\n",
        "## Preparing datasets\n",
        "\n",
        "We're finally ready to dip our toes into PyTorch! \n",
        "\n",
        "Let's start by defining a class `Dataset` which effectively wraps the `torch.utils.data.Dataset` class to fit our sequence classifier. Then we define an instance of `Dataset` for each of the three training, validation, and testing datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQcqg64eitrx"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = Dataset(encoded_text['train'], train_labels)\n",
        "val_dataset = Dataset(encoded_text['val'], val_labels)\n",
        "test_dataset = Dataset(encoded_text['test'], test_labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCB98dBfzgnh"
      },
      "source": [
        "Here are a couple of functions to help with monitoring the progress of model training. \n",
        "\n",
        "* `accuracy` - returns the accuracy of input predictions `pred`, given true labels `labels`.\n",
        "* `format_time` - takes a time in seconds and returns a string `hh:mm:ss`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-eIdFoW9Xb"
      },
      "source": [
        "def accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHPCjJK9EcH_"
      },
      "source": [
        "Now we set ALL the random seeds (for reproducibility) and set `device` to `cuda` if a GPU is available, otherwise we will use the CPU. Please not that the CPU could be several orders of magnitude slower than a  GPU in model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkV5vk1Di70y"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW3ClBHUFmJI"
      },
      "source": [
        "Finally we set the `batch_size` that the PyTorch `DataLoader` will use to create training mini-batches ([see this](https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data) for why min-batches are used). I chose `batch_size = 16`, but this is one of the model hyperparameters that can be tuned to optimize the classifier.\n",
        "\n",
        "We then create `DataLoader` instances for the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHKvinDBD0bM"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFTMeTD0HJdu"
      },
      "source": [
        "## Prepare model\n",
        "\n",
        "Now that we have our data, let's turn our attention to the model. We start by downlading the pre-trained `XLMRobertaForSequenceClassification` model. One thing to note is that we have to define how many labels we want our classifier to identify in the `num-labels` parameter - this sets the dimension of the output layer of the transformer model.\n",
        "\n",
        "Once the model in configured, we load it to `device` and set it to train mode.\n",
        "\n",
        "The last step is to define the optimizer and number of epochs we wish to use in model training. In this case we will use the [AdamW optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.AdamW) with a learning rate = `5e-5` and `10` epochs. While all of these parameters were the result of some trial and error, in practice, there are more programatic ways to arrive at them. [This tutorial](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) on hyperparameter tuning in PyTorch may be a good starting point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236,
          "referenced_widgets": [
            "9d45895a599b42b08f61bfc941f5a37f",
            "7ecee594708a4e8ba60cfd16b44381e6",
            "cbd1edf0441f451a8f8b08a745778c1b",
            "4438200d4dce4d7e8563988deec47c99",
            "f41e16e2e0134de4b00cbd4b656d22f6",
            "719cb8d737444f57822d890b92a2acda",
            "d0eaf3e5dbf44acfb5e659793bf2119f",
            "6be46dba46ea4c6d9548186439a85dc0",
            "e685101bd7634dfda5859ee0fab7a7ca",
            "722e929e8408453ea000fd254a03ccc7",
            "c3fbb8956b324375a03e1fdf2645e9c4",
            "ba926687ca0e417d98be71f97f4684a2",
            "08556bae90c44ce596f54818eccd355f",
            "9d478873a23948e7990985d8a7315f12",
            "59f9db8cbf424a69914aadedf1fbe9e9",
            "3af0995287fc4caf98019f7ab4926b26"
          ]
        },
        "id": "tQvyDzOeDQ-r",
        "outputId": "914869bc-9e1f-4e42-c462-9c891bf3419b"
      },
      "source": [
        "from transformers import XLMRobertaForSequenceClassification, AdamW\n",
        "\n",
        "print(\"Downloading XLM-R model from Hugging Face...\")\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base',\n",
        "          num_labels = len(set(labels)),\n",
        "          output_attentions = False,\n",
        "          output_hidden_states = False)\n",
        "model.to(device)\n",
        "model.train();\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 10\n",
        "loss_values = []"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading XLM-R model from Hugging Face...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d45895a599b42b08f61bfc941f5a37f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_‚Ä¶"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e685101bd7634dfda5859ee0fab7a7ca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr‚Ä¶"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0niuHF5TKtZ_"
      },
      "source": [
        "## Model training\n",
        "\n",
        "What follows is a standard PyTorch implementation of model training via backpropogation. For each training epoch, the code outputs the average training loss as well as the calculated accuracy on the validation dataset.\n",
        "\n",
        "We also checkpoint the trained model after every epoch - this can be done more often (e.g. every n steps) for longer training times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_33_BzAttlCj",
        "outputId": "3ba3fefa-92ea-4188-a414-6b52f9e91a19"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    print(\"\")\n",
        "    print(f'======== Epoch {epoch+1} / {epochs} ========')\n",
        "    print('Training...')\n",
        "    total_loss = 0\n",
        "    t0 = time.time()\n",
        "    update_interval = len(train_loader)//5\n",
        "    for step, batch in enumerate(train_loader):\n",
        "      if step % update_interval == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print(f'  Batch {step:>5,}  of  {len(train_loader):>5,}.    Elapsed: {elapsed}.')\n",
        "      optim.zero_grad()\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      outputs = model(input_ids, \n",
        "                      attention_mask=attention_mask, \n",
        "                      labels=labels)\n",
        "      \n",
        "      loss = outputs[0]\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "\n",
        "      optim.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    loss_values.append(avg_train_loss) \n",
        "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    nb_eval_steps, eval_accuracy = 0, 0\n",
        "    print(\"\\nRunning Validation...\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch in val_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      with torch.no_grad(): \n",
        "        outputs = outputs = model(input_ids, \n",
        "                                  attention_mask=attention_mask, \n",
        "                                  token_type_ids=None) \n",
        "\n",
        "      logits = outputs[0]                   \n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "      eval_accuracy += accuracy(logits, label_ids)\n",
        "      nb_eval_steps += 1\n",
        "\n",
        "    print(\"\\n  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    torch.save(model, f\"xlmr_multilingual_categorization_bbc_datastet_{epoch+1}_epochs.pt\")\n",
        "\n",
        "    \n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:20.\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epoch took: 0:00:21\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epoch took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.94\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epoch took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epoch took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.95\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epoch took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.95\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epoch took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.92\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epoch took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.74\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:20.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epoch took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.95\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epoch took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.96\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    17  of     89.    Elapsed: 0:00:04.\n",
            "  Batch    34  of     89.    Elapsed: 0:00:08.\n",
            "  Batch    51  of     89.    Elapsed: 0:00:12.\n",
            "  Batch    68  of     89.    Elapsed: 0:00:16.\n",
            "  Batch    85  of     89.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epoch took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Accuracy: 0.96\n",
            "  Validation took: 0:00:01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScXX6Ju5lht9"
      },
      "source": [
        "## Results\n",
        "\n",
        "As per the training progress above, we get as high as `0.98` accuracy on the validation set, and the average training loss drops to `0.01`. Not bad at all!\n",
        "\n",
        "Lets plot the progress of the average training loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "j00Kfm_iFLcQ",
        "outputId": "c400eb5c-03c4-4970-fc7a-d595d634ce94"
      },
      "source": [
        "import plotly.express as px\n",
        "f = pd.DataFrame(loss_values)\n",
        "f.columns=['Loss']\n",
        "fig = px.line(f, x=f.index + 1, y=f.Loss)\n",
        "fig.update_layout(title='Training loss of the Model',\n",
        "                   xaxis_title='Epoch',\n",
        "                   yaxis_title='Loss')\n",
        "fig.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"87ab70ec-711d-43ef-b6b7-1dcd451e8a87\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"87ab70ec-711d-43ef-b6b7-1dcd451e8a87\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '87ab70ec-711d-43ef-b6b7-1dcd451e8a87',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"x=%{x}<br>Loss=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"xaxis\": \"x\", \"y\": [0.6360066187138973, 0.17335818885836038, 0.06684582376737525, 0.062172800360059136, 0.06557682926473574, 0.043458319566045164, 0.11696690594086821, 0.08723114133979916, 0.07242428005955527, 0.04952056418183479], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Training loss of the Model\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('87ab70ec-711d-43ef-b6b7-1dcd451e8a87');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vKL71QZKnEH"
      },
      "source": [
        "## Testing the model\n",
        "\n",
        "We now have a trained model! In fact, we have a few. Let's pick the one that corresponds to the lowest average loss (as in the plot above) and see how it does with our test dataset.\n",
        "\n",
        "The function `predict` is essentially the same as the training code we used previously, without the outer (epoch) loop. When classifying new data, the model outputs a series of [logits](https://en.wikipedia.org/wiki/Logit) which represents the relative probability of each category. i.e. the largest output value corresponds to the label that the model deems most probable for the given text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwWTaykHa_Xa"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "def predict(data_loader, model):\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  for batch in data_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    with torch.no_grad(): \n",
        "      outputs = outputs = model(input_ids, \n",
        "                                attention_mask=attention_mask, \n",
        "                                token_type_ids=None) \n",
        "\n",
        "    logits = outputs[0]                   \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  return predictions, true_labels\n",
        "\n",
        "model_path = glob(f\"xlmr_*{np.argmin(loss_values)+1}_epochs.pt\")[0]\n",
        "model = torch.load(model_path)\n",
        "model.eval()\n",
        "\n",
        "predictions, true_labels = predict(test_loader, model)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B2Kp-UjXJN4"
      },
      "source": [
        "And here is what the predicitons look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFD32iUyXQdT",
        "outputId": "cd43377a-a9fe-4ecc-fad8-644c22a592df"
      },
      "source": [
        "print(\"# of predictions =\", sum(len(p) for p in predictions))\n",
        "print(\"# of logits per prediction =\", len(predictions[0][0]))\n",
        "print('\\n', predictions[0][0:5])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of predictions = 445\n",
            "# of logits per prediction = 5\n",
            "\n",
            " [[-0.5814546   0.12978259 -0.78264946 -1.2094038   1.9385245 ]\n",
            " [-0.4160801  -0.8368629  -1.5777658  -2.4954176   5.7345533 ]\n",
            " [-1.4676574  -1.0874201  -1.6271573   6.6566863  -1.6515472 ]\n",
            " [-0.7357026  -0.96731544 -1.0665618  -2.6714253   5.478526  ]\n",
            " [-1.7571563  -0.7775552   5.796031   -1.4086983  -2.1100788 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiDn5HRCXiNv"
      },
      "source": [
        "So, we have a vector of five logits (or log-odds) for each text input, where each logit corresponds to one of the topic categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fjCs4_LJKRF"
      },
      "source": [
        "### Assessing predicitons\n",
        "\n",
        "Yay, predictions! But how good are our predictions? The following code finds the category which corresponds to the largest logit in each prediciton vector and compares it against the true category to produce a classification report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tinfiI7Ijsn6",
        "outputId": "a745a9ba-d44d-4b9e-d768-f49d3e28c11e"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "pred_cats1 = [np.argmax(predictions[i], axis=1).flatten() for i in range(len(true_labels))]\n",
        "pred_cats1 = np.concatenate(pred_cats1).ravel()\n",
        "pred_cats1 = le.inverse_transform(pred_cats1)\n",
        "\n",
        "true_cats = le.inverse_transform(np.concatenate(true_labels).ravel())\n",
        "\n",
        "print(classification_report(true_cats, pred_cats1))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.89      0.93      0.91       115\n",
            "entertainment       1.00      0.92      0.96        71\n",
            "     politics       0.85      0.98      0.91        81\n",
            "        sport       1.00      0.97      0.98        94\n",
            "         tech       0.95      0.86      0.90        84\n",
            "\n",
            "     accuracy                           0.93       445\n",
            "    macro avg       0.94      0.93      0.93       445\n",
            " weighted avg       0.93      0.93      0.93       445\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLEXyapxY44A"
      },
      "source": [
        "As you can see from the `accuracy` values above, the model performs almost as well on the testing dataset as on the training dataset (ah, sweet, sweet generalization).\n",
        "\n",
        "Let's do one more bit of analysis. Now, we grab the 2nd largest value in each prediction vector and see how well it matches the true category for *input text that was categorized incorrectly*. i.e. are there ambiguous cases where the text could reasonably be classified into multiple categories?\n",
        "\n",
        "The code below gets the `k`-th largest logit in each vector, if that `k-th logit > 0.0` (since logit value of `0.0` corresponds to `50%` probability). In this case, let's just look at the topic category correponding to the 2nd largest logit value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un-cniGQSCvr"
      },
      "source": [
        "def kth_largest(predictions_i, k):\n",
        "  kth = np.argsort(-predictions_i, axis = 1)[:,k-1]\n",
        "  preds = le.inverse_transform(list(kth))\n",
        "  if k != 1:\n",
        "    mask = np.where(np.bincount(np.where(predictions_i > 0)[0]) != k)[0]\n",
        "    preds = np.array([None if i in mask else val for i, val in enumerate(preds)])\n",
        "\n",
        "  return preds\n",
        "\n",
        "pred_cats2 = [kth_largest(predictions[i], 2).flatten() for i in range(len(predictions))]\n",
        "pred_cats2 = np.concatenate(pred_cats2).ravel()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIdfDvdOjFQk"
      },
      "source": [
        "Now, lets look at the classification report comparing the actual categories of the input instances that were missclassified, to the 2nd most probable predicted categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JslNv9F7a3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb928d3c-d7aa-4c56-e7c3-c11ce07d1b84"
      },
      "source": [
        "res = pd.DataFrame({'actual':true_cats,\n",
        "                    'prediction_1':pred_cats1, \n",
        "                    'prediction_2':pred_cats2, \n",
        "                    'text':test_texts})\n",
        "\n",
        "subres = res[(~res['prediction_2'].isna()) & (res['prediction_1']!=res['actual'])]\n",
        "\n",
        "print(classification_report(subres['actual'], subres['prediction_2']))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.62      1.00      0.77         5\n",
            "entertainment       1.00      0.60      0.75         5\n",
            "     politics       0.33      1.00      0.50         1\n",
            "        sport       0.50      1.00      0.67         1\n",
            "         tech       1.00      0.50      0.67         8\n",
            "\n",
            "     accuracy                           0.70        20\n",
            "    macro avg       0.69      0.82      0.67        20\n",
            " weighted avg       0.85      0.70      0.70        20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JRXM5mOmuab"
      },
      "source": [
        "As you can see, the 2nd most probable prediction (with logit value > `0.0`) does seem to consistently match the actual category for the miscategorized input text. However, this sample size is rather small and as such, we should take this result with a grain of salt.\n",
        "\n",
        "That said, let's look at a sample of the text that was miscategorized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW-kO_jVlQ5B",
        "outputId": "3ee6dedd-f364-4757-ca39-b4d4038b2a6b"
      },
      "source": [
        "for i, row in subres.sample(5).iterrows():\n",
        "  print(f\"Text: {row['text'][0:120]}...\")\n",
        "  print(f\"Actual category: {row['actual']}\")\n",
        "  print(f\"Top prediction: {row['prediction_1']}\")\n",
        "  print(f\"2nd prediction: {row['prediction_2']}\\n\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: Prop Jones ready for hard graft  Adam Jones says the Wales forwards are determined to set the perfect attacking platform...\n",
            "Actual category: sport\n",
            "Top prediction: politics\n",
            "2nd prediction: sport\n",
            "\n",
            "Text: Arthur Hailey: King of the bestsellers  Novelist Arthur Hailey, who has died at the age of 84, was known for his bestsel...\n",
            "Actual category: entertainment\n",
            "Top prediction: politics\n",
            "2nd prediction: sport\n",
            "\n",
            "Text: Bets off after Big Brother 'leak'  A bookmaker has stopped taking bets on Celebrity Big Brother after claiming \"sensitiv...\n",
            "Actual category: entertainment\n",
            "Top prediction: politics\n",
            "2nd prediction: business\n",
            "\n",
            "Text: BT boosts its broadband packages  British Telecom has said it will double the broadband speeds of most of its home and b...\n",
            "Actual category: tech\n",
            "Top prediction: business\n",
            "2nd prediction: tech\n",
            "\n",
            "Text: Millions to miss out on the net  By 2025, 40% of the UK's population will still be without internet access at home, says...\n",
            "Actual category: tech\n",
            "Top prediction: business\n",
            "2nd prediction: tech\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCUmMD0ZuXiD"
      },
      "source": [
        "From the above, it can be noted that while some of the examples are slightly ambiguous in their category, there are a few instances where the predictions are flatout incorrect. Depending on your intended application, the `k-th` largest logit approach may add unnecessary complexity for a marginal improvement in accuracy.\n",
        "\n",
        "For now, let's see what effect using the 2nd predictions has on the accuracy of the test dataset predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dWmQ6M3kKy9",
        "outputId": "f46c5eb3-472c-4e04-f64c-6ae1dde4674c"
      },
      "source": [
        "p1 = [ p==l for (p,l) in zip(pred_cats1, true_cats)]\n",
        "p2 = [ p==l for (p,l) in zip(pred_cats2, true_cats)]\n",
        "acc1 = sum(p1)/len(p1)\n",
        "acc12 = sum([ p1 | p2 for (p1,p2) in zip(p1, p2)])/len(p1)\n",
        "\n",
        "print(f\"Accuracy of first prediction = {acc1:.2}\")\n",
        "print(f\"Accuracy of first or second prediction = {acc12:.2}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of first prediction = 0.93\n",
            "Accuracy of first or second prediction = 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0X1J1a99OwL"
      },
      "source": [
        "There we go! A 3% improvement.\n",
        "\n",
        "I mentioned earlier that XLM-RoBERTa is a multilingual model; however, so far, we have only tested the model on English language news. In the next post, we'll explore how to use our trained model to translate completely new news text in other langauges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVEGMXdh91am"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}